19:51:52 From  Victor Chavarria gl  to  Hosts and panelists:
	Good mornin
19:51:59 From  Victor Chavarria gl  to  Hosts and panelists:
	g*
19:53:28 From  Anthony W Thompson gl  to  Everyone:
	Good day from central Iowa!
19:53:34 From  Vijay Goswami gl  to  Everyone:
	yes
19:53:35 From  Dandan Kowarsch gl  to  Everyone:
	yes
19:53:36 From  FABIANA P NOVELLO gl  to  Everyone:
	yes
19:53:40 From  YAN MING HU gl  to  Everyone:
	yes
19:53:42 From  Saradha Ravi gl  to  Hosts and panelists:
	yes
19:53:42 From  Vijay Goswami gl  to  Everyone:
	Good Morning
19:53:44 From  Dominic H. Goodall gl  to  Everyone:
	Good morning!
19:53:48 From  Satyanarayana Reddy Karri gl  to  Everyone:
	Good morning!
19:53:49 From  YAN MING HU gl  to  Everyone:
	good morning
19:53:49 From  Morgan gl  to  Everyone:
	yes
19:53:59 From  Vera Pfeiffer gl  to  Everyone:
	Good morning!
19:54:01 From  Cristina Chiquinquir√° Hern√°ndez Labrador gl  to  Hosts and panelists:
	Yes I can hear!
19:54:05 From  Dandan Kowarsch gl  to  Everyone:
	good morning!
19:54:57 From  Mohammad Aleem gl  to  Everyone:
	Good morning, Professor
19:55:14 From  Timo Huovinen gl  to  Everyone:
	Hi Dr Tsitsiklis, I think the date is wrong on that slide
19:55:19 From  Will McGuire gl  to  Everyone:
	good morning from BC
19:55:48 From  RAVI KUMAR TOLETY gl  to  Everyone:
	Good Morning Professor and all..
19:55:54 From  Diallo Bocar Elimane gl  to  Everyone:
	Hello Everyone
19:56:02 From  Michael Allerheiligen gl  to  Hosts and panelists:
	Good morning!
19:56:04 From  neil mody gl  to  Hosts and panelists:
	Good morning
19:56:05 From  Pallavi Kawale gl  to  Everyone:
	Good morning everyone
19:56:07 From  Andy Mak gl  to  Everyone:
	Good morning everyone
19:56:10 From  Reto Voegeli gl  to  Everyone:
	Hello everyone
19:56:50 From  Oktay Selcuk gl  to  Everyone:
	hi everyone
19:56:57 From  Chen Wei Ku gl  to  Everyone:
	Good Morning
19:57:20 From  Blessy Joy Chamaparampil gl  to  Everyone:
	Good Morning everyone
19:57:29 From  Fedor Galstyan gl  to  Hosts and panelists:
	Good morning!
19:57:36 From  Kevin Humbles gl  to  Everyone:
	Good morning
19:57:40 From  Blessy Joy Chamaparampil gl  to  Everyone:
	yes
19:57:41 From  Oktay Selcuk gl  to  Everyone:
	yes
19:57:42 From  Emma Leibfried gl  to  Everyone:
	Yes we can 
19:57:42 From  Justin Stokes gl  to  Everyone:
	yes
19:57:43 From  Andy Mak gl  to  Everyone:
	Yes
19:57:43 From  Kevin Humbles gl  to  Everyone:
	Yes
19:57:44 From  Pedro Manuel Rossi Mercado gl  to  Everyone:
	Good Morning, yes
19:57:44 From  Vijay Goswami gl  to  Everyone:
	yes
19:57:45 From  Joseph W. Kinsella gl  to  Everyone:
	y
19:57:46 From  RAVI KUMAR TOLETY gl  to  Everyone:
	yes
19:57:47 From  Lucy Edosomwan gl  to  Hosts and panelists:
	Yes
19:57:49 From  Pallavi Kawale gl  to  Everyone:
	yes
19:57:49 From  Saradha Ravi gl  to  Hosts and panelists:
	yes
19:57:50 From  Tran Phan gl  to  Everyone:
	yes
19:57:51 From  Harold Stamateris gl  to  Everyone:
	yes can see and hear
19:57:52 From  Zuhair Nara gl  to  Everyone:
	hello
19:57:53 From  LAUREL KATHY ANN SCOTT gl  to  Everyone:
	good morning, yes seeing and hearing
19:57:55 From  Lucy Edosomwan gl  to  Hosts and panelists:
	Good morning from Atlanta ü•∞
19:57:56 From  Zuhair Nara gl  to  Everyone:
	all good
19:58:01 From  My Coyne gl  to  Everyone:
	Yes, I can hear and see the slide
19:58:55 From  Jennifer Pye gl  to  Everyone:
	Good Morning from Phoenix
19:58:59 From  Chris Flood gl  to  Everyone:
	good morning
19:59:04 From  Jacqueline gl  to  Everyone:
	Good morning everyone from Boston!
19:59:16 From  MAHZABEEN RAHMAN gl  to  Everyone:
	Good morning!
19:59:16 From  Viktoriya Olari gl  to  Hosts and panelists:
	Good afternoon from Berlin
19:59:20 From  Paula Iglesias Ot√°rola gl  to  Everyone:
	Good morning!
19:59:24 From  Dr. Reginald Terrell gl  to  Everyone:
	Yes, loud and clear and Thank you in advance
19:59:33 From  William Ricci gl  to  Everyone:
	Good morning!
19:59:48 From  Srikanth Panchavati gl  to  Hosts and panelists:
	Good morning!
19:59:52 From  Abdelaziz Aitouchen gl  to  Everyone:
	good morning from Phoenix
19:59:52 From  Mohammad Ayub gl  to  Hosts and panelists:
	Good morning
20:00:01 From  Cheslan Simpson gl  to  Everyone:
	Good morning.
20:00:06 From  Srikanth Panchavati gl  to  Everyone:
	Good morning from Toronto!
20:00:12 From  Kuldeep Rawat gl  to  Everyone:
	Good Morning from NC!
20:00:17 From  Linda Diec gl  to  Hosts and panelists:
	Good morning
20:00:20 From  MARIA SUSANA REZZONICO gl  to  Everyone:
	Good morning
20:00:27 From  Chris Lieberman gl  to  Hosts and panelists:
	Hello everyone.
20:00:43 From  Mohamad Khachab gl  to  Everyone:
	Good Morning everyone.
20:00:50 From  Fatemeh Shahriari gl  to  Hosts and panelists:
	Hello
20:00:52 From  NING LI gl  to  Everyone:
	Morning from Boston!
20:00:59 From  Adithi Mohan gl  to  Everyone:
	morning
20:01:09 From  Pornthip Suyasith gl  to  Hosts and panelists:
	Hi fro, Boston!
20:01:15 From  William Corwin gl  to  Everyone:
	Good morning friends!
20:01:27 From  Chari-Jo Johnson gl  to  Everyone:
	Good morning
20:01:38 From  Reena Choudhary gl  to  Everyone:
	good morning
20:01:43 From  Sridhar R. Papagari Sangareddy gl  to  Hosts and panelists:
	Good Morning All!!
20:01:44 From  Shan Siddiqui gl  to  Everyone:
	Good morning!
20:01:56 From  venkata Ratna Priya moganti gl  to  Hosts and panelists:
	Good Morning Dr John
20:01:56 From  Dino Cehic gl  to  Everyone:
	hello
20:03:01 From  Chris Flood gl  to  Everyone:
	not ot be confused with the world being flat!
20:05:52 From  Mohammed Ishaque Ibrahim gl  to  Everyone:
	there is lot of disturbance in voice, is everyone else able to hear clearly?
20:06:00 From  Dayna Levy gl  to  Everyone:
	I hear clearly
20:06:01 From  Chris Flood gl  to  Everyone:
	clear
20:06:02 From  Eduardo Brandao de Souza Mendes gl  to  Everyone:
	I hear fine here
20:06:06 From  Pedro Manuel Rossi Mercado gl  to  Everyone:
	clear
20:06:07 From  Arnold Estrada gl  to  Everyone:
	i hear clearly
20:06:07 From  Andy Mak gl  to  Everyone:
	I hear clearly
20:06:07 From  Manidip Ghosh gl  to  Everyone:
	I am able to hear fine
20:06:08 From  Joby Schaffer gl  to  Everyone:
	Clear
20:06:19 From  Cristina Chiquinquir√° Hern√°ndez Labrador gl  to  Hosts and panelists:
	Also I can hear clearly
20:06:22 From  Jacqueline gl  to  Everyone:
	very clear here
20:06:31 From  Nageswara Rao Biradhar gl  to  Everyone:
	Please explain difference between Noise and Variance with an example
20:06:40 From  Rajesh Munirathnam gl  to  Everyone:
	Hi
20:07:01 From  Chris Flood gl  to  Everyone:
	where are Monday's annotated lecture slides?
20:08:17 From  Diljot Chhina gl  to  Everyone:
	Just to understand heterosekdascity implies our independent variable is correlated to our W, so we have not captured all Xs correctly in our model?
20:08:34 From  Govinda Villasa Lopez Garza gl  to  Everyone:
	x2
20:08:34 From  Thanh Dang gl  to  Hosts and panelists:
	How does the weight help with making prediction?
20:08:55 From  Mohammad Nazif Faqiry gl  to  Everyone:
	So any cone shaped data would mean heteroschedasticity?
20:10:17 From  Nageswara Rao Biradhar gl  to  Everyone:
	What is the difference between noise and variance?
20:10:26 From  Saradha Ravi gl  to  Hosts and panelists:
	aren‚Äôt theta also weights in the model for each component..
20:10:32 From  HARIISH UPPILI gl  to  Everyone:
	Can u show the previous slide
20:11:11 From  Lucy Edosomwan gl  to  Everyone:
	Harish the slides are available under Courses Machine Learning
20:11:11 From  Saradha Ravi gl  to  Hosts and panelists:
	got it.thanks
20:11:32 From  Lucy Edosomwan gl  to  Everyone:
	Hariish*
20:13:49 From  [GL-TA] Prateek Dhokwal  to  Everyone:
	@Thanh Q: How does the weight help with making prediction?	A: The prediction Is the product of the coefficients with attributes i.e. Y hat = (Theta hat)*X
20:14:14 From  Diljot Chhina gl  to  Everyone:
	thats a very interesting application of PCA!
20:15:00 From  John Rogers gl  to  Hosts and panelists:
	Chocolate = intelligence!
20:15:13 From  Srikanth Panchavati gl  to  Everyone:
	üôÇ
20:15:13 From  Nirupana S Natarajan gl  to  Everyone:
	yes
20:15:40 From  Chris Lieberman gl  to  Hosts and panelists:
	I‚Äôm literally eating chocolate as you teach this!!
20:15:47 From  Maristela Monteiro gl  to  Everyone:
	It may correlate with wealth too
20:15:53 From  Pornthip Suyasith gl  to  Everyone:
	Buy some chocolates now! lol
20:15:59 From  Brandi Bax gl  to  Everyone:
	This is why we need to eat chocolate. It's science.
20:16:08 From  Lucy Edosomwan gl  to  Everyone:
	appears biased
20:16:23 From  Sergio Bracho Argotte gl  to  Everyone:
	spurious relation
20:16:30 From  Eduardo Brandao de Souza Mendes gl  to  Everyone:
	I live in Switzerland... Shall I be waiting for my nobel prize üòúüòÇ?
20:16:43 From  Maristela Monteiro gl  to  Everyone:
	correlation is not causation...!
20:17:00 From  Dr. Reginald Terrell gl  to  Everyone:
	I'm going to buy some chocolate to help me understand.
20:17:03 From  Sunil Acharya gl  to  Everyone:
	only if you are eating enough Chocolate!
20:17:04 From  Nageswara Rao Biradhar gl  to  Everyone:
	@Eduardo - You have better probability though
20:17:08 From  Nitin Goalla gl  to  Everyone:
	latent variables
20:17:09 From  [GL-TA] Rajeev  to  Everyone:
	@Maristel, No correlation is not causation. It is different from that.
20:17:11 From  Lucy Edosomwan gl  to  Everyone:
	which is top 10 economy in the world - Swiss
20:17:14 From  Maristela Monteiro gl  to  Everyone:
	Switzerland happens to be the HQ of Nestle
20:17:33 From  Al Ganeshkumar gl  to  Everyone:
	selling more choclate causes wealth
20:18:27 From  Richard Daily gl  to  Everyone:
	Belgium has the best chocolate but they're not very far up the slope
20:18:41 From  Diljot Chhina gl  to  Everyone:
	Business insider is notorious for headlines like this
20:18:53 From  William Corwin gl  to  Everyone:
	Yes!!
20:18:54 From  Nageswara Rao Biradhar gl  to  Everyone:
	Click bait
20:19:06 From  Diljot Chhina gl  to  Everyone:
	^ yes!
20:19:44 From  Riaz U Ahmed gl  to  Everyone:
	how to figure out variable related to output is result of casual? Manually having better knowledge of data?
20:21:46 From  Michael Caldwell gl  to  Everyone:
	correlation, not causation
20:22:04 From  Nitin Goalla gl  to  Everyone:
	So, do we introduce latent variables in the regression?
20:22:04 From  Fernando Garcia Corona gl  to  Everyone:
	how is endogeneity different from multicollinearity?
20:22:23 From  Nirupana S Natarajan gl  to  Everyone:
	difference between correlation and causation
20:22:24 From  Adithi Mohan gl  to  Everyone:
	seeing trends
20:22:51 From  Adithi Mohan gl  to  Everyone:
	correlation is relationship. causation is output
20:23:14 From  Adithi Mohan gl  to  Everyone:
	causation = cause
20:23:41 From  Sunil Acharya gl  to  Everyone:
	It is also possible that the marketing department stays the same- but the new market region is different?
20:23:55 From  [GL-TA] Rajeev  to  Everyone:
	@Fernando, Q:how is endogeneity different from multicollinearity?	A: Multicollinearity is correlation between two independent variables, while endogeneity is the correlation between the error terms and the independent variables.
20:24:07 From  Fernando Garcia Corona gl  to  Everyone:
	thanks
20:24:07 From  SAURABH YASHWANT JOSHI gl  to  Hosts and panelists:
	What is the difference between endogeneity and multicollinearity?
20:24:15 From  [GL-TA] Jai  to  Everyone:
	@Fernando - Q: how is endogeneity different from multicollinearity?
	A: A correlation of one independent variable with another independent variable is known as multicollinearity. The correlation of an independent variable with the error term is known as endogeneity.
	
	
	
20:24:17 From  SAURABH YASHWANT JOSHI gl  to  Everyone:
	What is the difference between endogeneity and multicollinearity?
20:25:26 From  Mohamad Khachab gl  to  Everyone:
	Could we say endogeneity is unexplained effect.
20:25:49 From  Jesus Chavez gl  to  Everyone:
	that's where the human effect matters? or there is a mathematical model to detected causation?
20:26:36 From  Ilias Dodoulas gl  to  Everyone:
	is it necessary to run PCA in case of multicollinearity, otherwise the model will fail? or running with the original X's will still give a model, but maybe not a very good one?
20:27:16 From  Mark Braxton gl  to  Hosts and panelists:
	Can someone explain the maximum likelihood function?
20:29:30 From  MAHZABEEN RAHMAN gl  to  Everyone:
	Autocorrelation in data may cause endogeneity?
20:29:43 From  [GL-TA] Rajeev  to  Everyone:
	@Mark, Q:Can someone explain the maximum likelihood function?	A: Maximum likelihood function is a function that combines the likelihood of all the records(following some probability distribution) of a dataset to happen. It uses multiplication rule of probability to combine the probability of corresponding outcomes in each record.
20:29:46 From  HARIISH UPPILI gl  to  Everyone:
	So does endogenity uncover incomplete analysis ( missing key variables that actually impact correlation )
20:29:47 From  Alvin Kuo gl  to  Everyone:
	‚ÄúCommon in time series data‚Äù means ‚Äútime‚Äù should be part of the variables?
20:29:48 From  [GL-TA] Avijit  to  Everyone:
	@ Ilias   -Q:is it necessary to run PCA in case of multicollinearity, otherwise the model will fail? or running with the original X's will still give a model, but maybe not a very good one?	A: PCA is explicitly structured as to resolve multicollinearity among multiple variables. Given that, it is a very useful tool to get around multicollinearity.
20:30:02 From  Chris Flood gl  to  Everyone:
	why is it common when dealing with time-series data?
20:30:30 From  MAHZABEEN RAHMAN gl  to  Everyone:
	Due to autocorrelation?
20:31:02 From  Chris Flood gl  to  Everyone:
	all my work will be in time series data
20:31:08 From  Josh Goldberg gl  to  Everyone:
	More in academia than in industry, instrumental variables are used to solve endogeneity
20:31:29 From  My Coyne gl  to  Everyone:
	How do you detect endogeneity, in order to mitigate?
20:31:36 From  Deepak Gaikwad gl  to  Everyone:
	is there an overlap between the 2 terms -  heteroscedasticity and endogeneity?
20:33:26 From  [GL-TA] Rajeev  to  Everyone:
	@My, one can check the correlation between the independent variables and the error terms treating them as two different variables.
20:33:31 From  Jesus Chavez gl  to  Everyone:
	that's a lot of work, what about when you have huge datasets, it's a manual effort to add new features
20:33:59 From  Mohammad Nazif Faqiry gl  to  Everyone:
	Will the data be useless for regression if we don't have one of the important categorical data?
20:34:01 From  Nageswara Rao Biradhar gl  to  Everyone:
	Is that the reason we create dummy variables for categorical variables ? To mitigate Endogeneity?
20:34:25 From  Jesus Chavez gl  to  Everyone:
	not really, that dummy variables or one hot encooding is just representation of numbers
20:34:40 From  Diljot Chhina gl  to  Everyone:
	dummy variables usually used to categorize string values in your dataset
20:34:57 From  Diljot Chhina gl  to  Everyone:
	For endogeneity should it always be binary for categorical
20:35:02 From  John W Pruitt gl  to  Hosts and panelists:
	Categorical variables are like booleans, true or false for purposes of calculation hence the 0 or 1.
20:35:37 From  Hongbin Liu gl  to  Everyone:
	What if there are more than 2 choices for a categorical var
20:35:52 From  Diljot Chhina gl  to  Everyone:
	^ ya I have a similar question like three groups of markets
20:36:08 From  Saradha Ravi gl  to  Hosts and panelists:
	how do we detect endogeneity ? in a model built before adding variables..?
20:36:19 From  Rahul Chugh gl  to  Everyone:
	even when you are taking 0s and 1s.. U and V might be correlated - that might lead to (XTX)-1 becoming infinity or large??
20:36:25 From  Chris Lieberman gl  to  Hosts and panelists:
	What is the technical term for replacing categorical variables with numericl variables such that the values used don‚Äôt bias the model?
20:36:56 From  John W Pruitt gl  to  Hosts and panelists:
	Then set up 3 different categorical variables and ensure you‚Äôre not categorizing each data point in more than one category.
20:37:27 From  [GL-TA] Avijit  to  Everyone:
	@Mahzabeen   Q:Autocorrelation in data may cause endogeneity?	A:Endogeneity can arise, when we have the lagged dependent variable in our regression model and simultaneously autocorrelation in disturbances.
20:37:53 From  Jesus Chavez gl  to  Everyone:
	Mitigating endogeneity is adding more variables or encode the categorical values? :P lol
20:38:08 From  Diljot Chhina gl  to  Everyone:
	I think the professor said don‚Äôt consider encoding
20:39:13 From  Hongbin Liu gl  to  Everyone:
	If there are 4 categories with a var, then we encode it with 2 columns each taking either 1 or 0?
20:40:43 From  Jesus Chavez gl  to  Everyone:
	yep actually, you will have 4 new columns with 1 or 0
20:41:41 From  Mohammad Aleem gl  to  Everyone:
	So we are converting/transforming to a linear model?
20:41:46 From  Chao Sun gl  to  Everyone:
	would log x2 and x2 have a very correlation that could cause multicollinearity issue?
20:41:55 From  Hongbin Liu gl  to  Everyone:
	What is the impact on the modeling ‚Äî now we have 4 columns associated with one feature
20:42:04 From  Pornthip Suyasith gl  to  Everyone:
	Why do we need non-linear function added to the model?
20:42:12 From  [GL-TA] Rajeev  to  Everyone:
	@Hongbin, Q:If there are 4 categories with a var, then we encode it with 2 columns each taking either 1 or 0?	A: There will be four columns belonging to every category. Each of them will contain 0 and 1.
20:42:55 From  Joseph Tidwell gl  to  Hosts and panelists:
	Would X1*X2 be an interaction between these two variables?
20:43:46 From  My Coyne gl  to  Everyone:
	Can you take log of Y and still have linear regression?
20:43:56 From  Pornthip Suyasith gl  to  Everyone:
	Thank you.
20:44:09 From  Nageswara Rao Biradhar gl  to  Everyone:
	@Coyne - Yes
20:44:31 From  Juan Bermudez gl  to  Everyone:
	Temperature and humidity
20:47:58 From  Al Ganeshkumar gl  to  Everyone:
	what is R^2
20:48:47 From  Paula Iglesias Ot√°rola gl  to  Everyone:
	rescale‚Ä¶ normalize or standardize? Is there a difference?
20:49:30 From  MAHZABEEN RAHMAN gl  to  Everyone:
	Which one should be considered, adjusted R sq or passing the Wald test?
20:49:43 From  Thanh Dang gl  to  Hosts and panelists:
	Can you provide an example of weighted least square algorithm? How do we pick the weight?
20:50:09 From  Svetlana Vo gl  to  Everyone:
	can you give some examples where scaling/ normalizing is necessary and where the regression will work just fine without transforming the data first?
20:50:26 From  Mukul Mondal gl  to  Everyone:
	What's the minimum number of data point count, we need to do this calculations?
20:50:31 From  Nikhil Kamma gl  to  Everyone:
	Why were we considering considering standard error inn slide 12, isn‚Äôt coefficient enough to reject null; hypothesis by comparing with significance level
20:51:04 From  Diallo Bocar Elimane gl  to  Everyone:
	I think power regression  y = a*x^b better than log regression because faster?
20:51:10 From  Diljot Chhina gl  to  Everyone:
	if we take the log of Y can we still account for outliers in our prediction model
20:51:36 From  Pornthip Suyasith gl  to  Everyone:
	R2 tell us to what extent the model could explain the variance of variable Y (dependent var). Mostly tell us in %.
20:51:57 From  Al Ganeshkumar gl  to  Everyone:
	when you add a categorical component like v on to linear predictor isn't it like just scaling the prediction curve by a scalar
20:52:00 From  Diljot Chhina gl  to  Everyone:
	the use case I am thinking of where log Y is used is bitcoin prices in the last 5 years
20:54:00 From  Isaac Flikier Zelkowicz gl  to  Hosts and panelists:
	would linear regression be your first choice to determine causality in a scenario of multifactorial causality?
20:54:08 From  Isaac Flikier Zelkowicz gl  to  Hosts and panelists:
	as in clinical trials
20:58:07 From  Al Ganeshkumar gl  to  Everyone:
	what is relationship between m and n
20:58:45 From  Jesus Chavez gl  to  Everyone:
	can you explain a little more why the other model is called bayessian approximation?
20:58:53 From  Mohammad Nazif Faqiry gl  to  Everyone:
	m is on certain parameters ( you don't want to regularize all thetas)
20:59:12 From  Chris Lieberman gl  to  Hosts and panelists:
	Can you talk about what a ‚Äúhyper parameter‚Äù is?  I‚Äôm confused by that term.
20:59:38 From  Mohammad Nazif Faqiry gl  to  Everyone:
	@Al
21:00:11 From  Yair Listokin gl  to  Everyone:
	What does using lasso or ridge regression do to standard errors of coefficient estimates?
21:00:24 From  Diljot Chhina gl  to  Everyone:
	What does sparsity mean
21:00:37 From  Eduardo Brandao de Souza Mendes gl  to  Everyone:
	Shall we scale our variables (such as a z-score) prior to running a linear regression (to be able to compare the coefficients significances) or not?
21:00:38 From  Diljot Chhina gl  to  Everyone:
	or sparsity structure
21:00:39 From  Saradha Ravi gl  to  Hosts and panelists:
	vector with mostly zero values
21:00:46 From  Josh Goldberg gl  to  Everyone:
	Do you drop the zeros in a subsequent iteration?
21:00:48 From  Wilberto W Montoya gl  to  Everyone:
	How close should be to ) to consider discard the components?
21:00:53 From  Wilberto W Montoya gl  to  Everyone:
	to 0
21:00:53 From  [GL-TA] Rajeev  to  Everyone:
	@Chris, Q:Can you talk about what a ‚Äúhyper parameter‚Äù is?  I‚Äôm confused by that term.	A: When we train a machine learning model, there are parameters that we are free to pass values of. Such parameters are called as hyperparameters. For example: alpha here is a hyperparameter. While applying regularization we can pass its value by our own choice.
21:01:35 From  Diallo Bocar Elimane gl  to  Everyone:
	We use alpha and theta to avoid overfitting why putting them to zero?
21:02:43 From  Saradha Ravi gl  to  Hosts and panelists:
	we force the theta values of the solution to 0 not alpha I guess.
21:03:21 From  Jesus Chavez gl  to  Everyone:
	good question...
21:08:49 From  [GL-TA] Prateek Dhokwal  to  Everyone:
	@Al Q: what is the relationship between m and n?	A: n is the number of record and m is the number of features.
21:11:40 From  SAURABH YASHWANT JOSHI gl  to  Hosts and panelists:
	Why too many features - result in noisy (Variance)?
21:11:41 From  Jesus Chavez gl  to  Everyone:
	does that (avoid many features) applies for when you have categorical features and transform it to 1/0?
21:16:15 From  Hongbin Liu gl  to  Everyone:
	I have the same question
21:17:55 From  Hongbin Liu gl  to  Everyone:
	Central Limit Theorem says 30 is magic number. Why it is 10 here
21:21:02 From  Nageswara Rao Biradhar gl  to  Everyone:
	Use hypothesis test again?
21:21:14 From  Jesus Chavez gl  to  Everyone:
	30? I thought it was 5% per each side
21:21:46 From  Estelle Pearl Yuzicappi gl  to  Everyone:
	Are talking about normal distribution?
21:22:05 From  Sunil Acharya gl  to  Everyone:
	I thin the CLT ~ 30 reference is for sample size
21:22:49 From  Yair Listokin gl  to  Everyone:
	Could you randomly choose a different validation set rather than setting aside a test set?
21:22:50 From  Jesus Chavez gl  to  Everyone:
	oh! ye
21:23:03 From  Francesco Boccardo gl  to  Hosts and panelists:
	Is the validation set a subset of the data?
21:23:21 From  MAHZABEEN RAHMAN gl  to  Everyone:
	What is the difference between test data and validation data?
21:23:29 From  [GL-TA] Rajeev  to  Everyone:
	@Francesco, Yes, it is a subset of the data
21:23:30 From  Jesus Chavez gl  to  Everyone:
	N=30 is good but when your data has a non-normal distribution, 100 samples is better
21:23:36 From  Chris Lieberman gl  to  Hosts and panelists:
	What %‚Äôs of the overall sample data are used for training, validation & test sets
21:23:48 From  Suresh Prathipati gl  to  Hosts and panelists:
	What features to keep and which features to throw away?
21:23:55 From  Francesco Boccardo gl  to  Hosts and panelists:
	Thank you
21:24:01 From  [GL-TA] Avijit  to  Everyone:
	@Chris It depends on the amount of data we have. Generally, we consider 60% data as a train set, 20% data as a validation set, and 20% as a test set.
21:24:11 From  Chris Lieberman gl  to  Hosts and panelists:
	Thanks
21:27:10 From  Martin Niehoff gl  to  Everyone:
	So which of the 1000 models to I finally pick?
21:27:24 From  Arnold Estrada gl  to  Everyone:
	but arent you coming up with different parameters every time?  So you arent validating a single model.  You are validating n models?
21:28:30 From  MAHZABEEN RAHMAN gl  to  Everyone:
	Yes, have the same question. And which one will be finally used?
21:29:12 From  Nageswara Rao Biradhar gl  to  Everyone:
	When you say best - is it the highest R squared value?
21:30:00 From  Ticiana Beqari gl  to  Everyone:
	how do we choose k
21:31:38 From  Yair Listokin gl  to  Everyone:
	and then you pick the regression method that has the lowest summary score?
21:31:47 From  Omar Fahmy gl  to  Hosts and panelists:
	How do you combine the 4 regressions you got in each of the k folds?
21:32:51 From  [GL-TA] Rajeev  to  Everyone:
	@Yair, Q:and then you pick the regression method that has the lowest summary score?	A: Yes, the model with lowest summary score is selected.
21:33:26 From  NING LI gl  to  Everyone:
	How do you get the green line?
21:33:34 From  NING LI gl  to  Everyone:
	Do you have to run LOOCV?
21:34:01 From  NING LI gl  to  Everyone:
	If you have to run it to get the green, then what‚Äôs the point of doing the K
21:34:04 From  Nikhil Kamma gl  to  Everyone:
	Why K = 10 was selected?
21:34:17 From  Amrita Dutta gl  to  Everyone:
	randomly
21:34:27 From  Mohammad Aleem gl  to  Everyone:
	Really?
21:34:53 From  Mohammad Aleem gl  to  Everyone:
	traditional value?
21:34:54 From  Will McGuire gl  to  Everyone:
	is it absolue 10 or 10%
21:35:05 From  Will McGuire gl  to  Everyone:
	absolute
21:35:20 From  [GL] Vishnu Subramanian  to  Everyone:
	@Will k = 10, as in 10-fold cross validation
21:35:42 From  Omar Fahmy gl  to  Hosts and panelists:
	In every k of the k folds you would be getting a different regression, how do you combine them? (Do you combine them?)
21:36:03 From  Diallo Bocar Elimane gl  to  Everyone:
	If validation is different to test than what is validation how we can perform them?
21:36:17 From  Omar Fahmy gl  to  Everyone:
	In every k of the k folds you would be getting a different regression, how do you combine them? (Do you combine them?)
21:36:26 From  NING LI gl  to  Everyone:
	Again, if we have to run the LOOCV to get the green value in order to pick a value for K-fold, then how can K-fold help.
21:37:04 From  HARIISH UPPILI gl  to  Everyone:
	How do you decide the interval for K fold ?
21:37:21 From  Diallo Bocar Elimane gl  to  Everyone:
	Thanks
21:37:31 From  Nikhil Kamma gl  to  Everyone:
	When you say different methods, is it different parameters/features or different techniques other than regression
21:37:33 From  [GL] Vishnu Subramanian  to  Everyone:
	@Ning The figure was a reference to a general test done on an experiment, which suggested that k = 10 is a good compromise. In any new problem statement, you wouldn't need to run LOOCV again
21:37:52 From  NING LI gl  to  Everyone:
	thx
21:41:46 From  Saradha Ravi gl  to  Hosts and panelists:
	sampling
21:42:50 From  Yair Listokin gl  to  Everyone:
	Can simulation tell us anything about bias?
21:43:25 From  Jesus Chavez gl  to  Everyone:
	These "synthetic dataset" are created from the population?
21:43:48 From  Nageswara Rao Biradhar gl  to  Everyone:
	Synthetic data is from X right? Not the theta?
21:43:54 From  MAHZABEEN RAHMAN gl  to  Everyone:
	How to generate the synthetic data ? Depending on the true theta?
21:44:42 From  Victor Chavarria gl  to  Hosts and panelists:
	Get one point from the hat and put it back, multiple times‚Ä¶
21:45:15 From  Victor Chavarria gl  to  Hosts and panelists:
	So each point has the same probability of being drawn
21:45:17 From  Brian A Sakarata gl  to  Everyone:
	What determines if the sample is large enough?
21:47:15 From  Prerna Mathur gl  to  Hosts and panelists:
	Can the entire sample be same
21:47:34 From  Jesus Chavez gl  to  Everyone:
	but original dataset size can't be the same as the simulated. I'm confused
21:48:09 From  Chris Lieberman gl  to  Hosts and panelists:
	Wouldn‚Äôt having duplicates of random variables (for ex. a, b, b, c) throw off the results?
21:48:10 From  Victor Chavarria gl  to  Everyone:
	@ Jesus, there can be duplicates in the new set
21:48:10 From  Nikhil Kamma gl  to  Everyone:
	How picking up something twice or thrice qualifies it as a different data
21:48:24 From  Jesus Chavez gl  to  Everyone:
	oh so they are new random variables created from the distribution of the original dataset
21:48:39 From  HARIISH UPPILI gl  to  Everyone:
	So we are creating new sample data sets based on the permutation /combinatioms of the original data set in bootstrapping ?
21:48:55 From  Sarah Ford gl  to  Everyone:
	yes, keep drawing from dataset with replacement
21:49:09 From  Victor Chavarria gl  to  Everyone:
	No, it is like take one from the hat, and put it back, so each point has the same probability of being drawn again
21:49:09 From  Nikhil Kamma gl  to  Everyone:
	Seems like almost a sleight of hand üôÇ
21:49:15 From  [GL-TA] Rajeev  to  Everyone:
	@Nageswara, Q:Synthetic data is from X right? Not the theta?	A: Yes, it needs X to create the synthetic data.
21:49:34 From  Nageswara Rao Biradhar gl  to  Everyone:
	thanks
21:49:45 From  Estelle Pearl Yuzicappi gl  to  Everyone:
	Good to know!
21:50:18 From  Mohammad Nazif Faqiry gl  to  Everyone:
	How many times (m) is a good number of times of drawing with replacement in bootstrapping?
21:50:31 From  Chao Sun gl  to  Everyone:
	Is Monte Carlo simulation some type of bootstrapping?
21:50:38 From  Fernando Garcia Corona gl  to  Everyone:
	does bootstrapping require a minimum amount of samples (n) ?
21:52:22 From  Yair Listokin gl  to  Everyone:
	What do we do with theta_ave? Is it a better estimate of theta than theta_hat? Can theta_ave be used to investigate bias?
21:52:45 From  Ben Zhang gl  to  Everyone:
	For the simulation method, how do you draw the new data set? Do you draw from a pdf of the new data points using the current model parameters?
21:53:16 From  Omar Fahmy gl  to  Everyone:
	is bootstrapping done on a labelled dataset?
21:53:17 From  ZIXUAN JIAO gl  to  Everyone:
	When running the original lasso, your theta has already been associated with a SE, right?
21:53:17 From  Josh Goldberg gl  to  Everyone:
	When you bootstrap are you jointly estimating all parameters in multivariate regression or one at a time?
21:53:52 From  Nageswara Rao Biradhar gl  to  Everyone:
	What is the takeaway from the calculated standard error of theta? Is there any acceptable range for that to be a good final theta?
21:54:02 From  John Paul Balucan gl  to  Everyone:
	in the kfolds, how did you decide 10 compared to 5 folds.. do we have to look at the mse
21:54:22 From  Martin Niehoff gl  to  Everyone:
	So bootstrapping is a way of simulating the sampling distributin?
21:55:57 From  Wilberto W Montoya gl  to  Everyone:
	Does Bootstraping requires size >= 30 to assure normal distribution of the sample?
21:56:06 From  Nageswara Rao Biradhar gl  to  Everyone:
	But theta is not in units ? How do we relate ?
21:58:34 From  Ben Zhang gl  to  Everyone:
	How do we generate the new data set in the simulation method. Is it through Monte Carlo, or should we draw from a model pdf of data point given the current theta^hat.
21:59:54 From  Sarah Ford gl  to  Everyone:
	@Ben in R IIRC there is a bootstrap function that has a sample command that will do this (sampling with replacement) for you. I'm sure we'll learn for Python :)
22:00:18 From  Josh Goldberg gl  to  Everyone:
	With the penalty terms, do you remove the terms that go to zero?
22:00:20 From  Nitin Goalla gl  to  Everyone:
	Thank you Professor
22:00:24 From  Fedor Galstyan gl  to  Hosts and panelists:
	Thank you!
22:00:25 From  Mohamad Khachab gl  to  Everyone:
	Thank you Professor.
22:00:27 From  Mariela Trigueros gl  to  Everyone:
	Thank you for the great class
22:00:29 From  Sarah Ford gl  to  Everyone:
	Thank you, professor!
22:00:32 From  Dandan Kowarsch gl  to  Everyone:
	Thanks prof. great lecture
22:00:34 From  Adriana Catalina Vazquez Ortiz gl  to  Everyone:
	Thank you Professor!
22:00:34 From  Wilberto W Montoya gl  to  Everyone:
	Thank you Professor
22:00:36 From  Nageswara Rao Biradhar gl  to  Everyone:
	Thank you Professor!
22:00:37 From  Hemant Mylavarapu gl  to  Everyone:
	Thank You very much indeed Professor
22:00:41 From  Victor Chavarria gl  to  Everyone:
	Thank you Dr.
22:00:41 From  Hongbin Liu gl  to  Everyone:
	Thank you Professor !
22:00:42 From  Prerna Mathur gl  to  Hosts and panelists:
	Thank you
22:00:42 From  Indira R Reddy gl  to  Everyone:
	Thank you Professor
22:00:44 From  RAVI KUMAR TOLETY gl  to  Everyone:
	Thank you Professor!!
22:00:45 From  Gulusan Erdogan-Ozgul gl  to  Everyone:
	Thank you
22:00:46 From  Diallo Bocar Elimane gl  to  Everyone:
	THank
22:00:46 From  Shajan Thomas gl  to  Everyone:
	Thank you Professor
22:00:52 From  Estelle Pearl Yuzicappi gl  to  Everyone:
	Thank you
22:00:54 From  Richard Daily gl  to  Everyone:
	Thank you!!
22:00:58 From  Reto Voegeli gl  to  Everyone:
	Thank zou
22:01:02 From  Juan Bermudez gl  to  Everyone:
	Thanks
22:01:04 From  Jacqueline gl  to  Everyone:
	Tha
22:01:21 From  Jacqueline gl  to  Everyone:
	Thank you!
22:02:21 From  Nikhil Kamma gl  to  Everyone:
	How taking duplicates makes it different data in case of bootstrapping
22:03:04 From  Chi-lieh Lin gl  to  Everyone:
	last week we had se = sigma/sqrt(n) where n is sample size, compared to the 2nd last slide would average of theta-hat = theta-hat/sqrt(m)?
22:03:39 From  Nageswara Rao Biradhar gl  to  Everyone:
	What do we mean by different models? Different features or different algorithmic functions in python?
22:03:46 From  Sunil Acharya gl  to  Everyone:
	@Vishnu: is it not more of probabilistic event rather tather than duplicates?
22:04:04 From  CRistina Barboi gl  to  Hosts and panelists:
	can you clarify the differences between methods of parameter assessment versus model assesment
22:05:11 From  Diljot Chhina gl  to  Everyone:
	What if your dataset has a lot of duplicates should we only look at unique values for bootstrapping?
22:05:45 From  Chao Sun gl  to  Everyone:
	How exactly you do  replacement? just generate some random number similar to your sample data?
22:06:23 From  ZIXUAN JIAO gl  to  Everyone:
	can we see that if you have closed form solution or say we know the formula, we have analytical solution, there is no point in using numerical simulation like boostrapping? But in reality, we are not sure about the assumption we make?
22:06:34 From  Chris Lieberman gl  to  Hosts and panelists:
	Is bootstrapping always done or more of a nice to do?
22:07:58 From  Mohammad Nazif Faqiry gl  to  Everyone:
	Is it the correlation between predictor and error or one of the features and the error?
22:08:40 From  [GL] Vishnu Subramanian  to  Everyone:
	@Mohammad The features are also called predictor variables
22:08:51 From  Hongbin Liu gl  to  Everyone:
	A question on terminology: when people say estimator it is theta,  when saying predictor it is X ?
22:09:31 From  ZIXUAN JIAO gl  to  Everyone:
	cov(X,e), predictors are features.
22:09:58 From  Diljot Chhina gl  to  Everyone:
	any links/books recommended for the robust methods to handle endogeneity?
22:11:53 From  Sunil Acharya gl  to  Everyone:
	So by synthetic data It is both {X} and {Y} and you had distributions for both {X} and {Y} ?
22:15:43 From  Glendon Brown gl  to  Everyone:
	Minor in philosophy Bradford?
22:15:44 From  Saradha Ravi gl  to  Hosts and panelists:
	do graphical models handle causality better?
22:15:48 From  MAHZABEEN RAHMAN gl  to  Everyone:
	There will be different estimates of model parameters  for different folds? Which one to select?
22:16:28 From  Sunil Acharya gl  to  Everyone:
	IT will be a great experiment for "chocolate allergies"
22:19:04 From  Sunil Acharya gl  to  Everyone:
	@Chris Flood, are you on the slack channel? I am also interested in the "Time Series Data"
22:19:18 From  Wilberto W Montoya gl  to  Everyone:
	Lasso is used to prediction accuracy of the model, looks like but can be also used to remove features
22:19:20 From  Hongbin Liu gl  to  Everyone:
	Looking forward to this pic
22:22:12 From  Mohammad Nazif Faqiry gl  to  Everyone:
	Julia is fast and efficient but not as rich as Python.
22:22:23 From  Chris Lieberman gl  to  Hosts and panelists:
	Is the benefit of neural nets vs machine learning, that in NN‚Äôs, you don‚Äôt have to do feature section‚Ä¶the algorithm does it for you?
22:22:30 From  Juan Avila gl  to  Everyone:
	and what do you think about R?
22:23:08 From  Chris Lieberman gl  to  Hosts and panelists:
	What about SAS?
22:23:19 From  Glendon Brown gl  to  Everyone:
	R isn't free though
22:23:20 From  Brandi Bax gl  to  Everyone:
	What did you use before R that made it feel intuitive?
22:23:29 From  Diljot Chhina gl  to  Everyone:
	Python seems to be an industry standard
22:23:40 From  Chris Lieberman gl  to  Everyone:
	R is free
22:23:46 From  Ilias Dodoulas gl  to  Everyone:
	matlab?
22:23:48 From  Chris Lieberman gl  to  Everyone:
	It‚Äôs open source
22:23:49 From  Nageswara Rao Biradhar gl  to  Everyone:
	Do we need to create new statistical methods in real world scenario or they are already available in Python and we just need to know which one to use?
22:24:50 From  Maria Isabel Rodriguez Torres gl  to  Everyone:
	Matlab?
22:25:17 From  Dominic H. Goodall gl  to  Everyone:
	What are your thoughts on Scala?
22:25:40 From  Mohamad Khachab gl  to  Everyone:
	Thank you for a great super session.
22:25:51 From  Chris Lieberman gl  to  Everyone:
	Scala is really good for distributed processing
22:26:14 From  Chris Lieberman gl  to  Everyone:
	Companies use Scala for production systems to allow for scale.
22:27:15 From  Sunil Acharya gl  to  Everyone:
	Does Bradford have a blog?
22:27:25 From  Chris Lieberman gl  to  Everyone:
	SAS is a private company so you have to pay for it.  I used to work for them
22:27:26 From  Diallo Bocar Elimane gl  to  Everyone:
	SPSS?
22:27:48 From  Nikhil Kamma gl  to  Everyone:
	Which one to use LinearRegression or statsmodel for coming up with a model
22:28:40 From  Chris Lieberman gl  to  Everyone:
	SAS is getting crushed by R and other open source analytical software
22:28:46 From  Mohammad Nazif Faqiry gl  to  Everyone:
	But first, Learn Python, :).
22:29:19 From  Reza Banazadeh gl  to  Hosts and panelists:
	Is there any Open Source machine learning project as a reference you can recommend to get a sense of large scale models in use?
22:29:48 From  frantz verella gl  to  Hosts and panelists:
	Thanks a lot
22:30:03 From  Chris Lieberman gl  to  Hosts and panelists:
	Can Brad speak to the major difference between machine learning (such as linear regression) vs Neural Networks?
22:31:06 From  Wilberto W Montoya gl  to  Everyone:
	Just a comment, Based on Kaggle 73% of data scientist use Jupiter Notebook in 2021 üòâ
22:32:01 From  Mukul Mondal gl  to  Everyone:
	Thank you all
22:32:01 From  Brian A Sakarata gl  to  Everyone:
	There are a lot of uses of linear regressions in court, even when they shouldn't be used.
22:32:02 From  Saradha Ravi gl  to  Hosts and panelists:
	thank you!
22:32:03 From  Nageswara Rao Biradhar gl  to  Everyone:
	Great!
22:32:05 From  Victor Chavarria gl  to  Everyone:
	Thank you
22:32:06 From  Christopher Carrero gl  to  Everyone:
	thank you!
22:32:07 From  Estelle Pearl Yuzicappi gl  to  Everyone:
	Thanks Brad!
22:32:07 From  Dino Cehic gl  to  Everyone:
	thanks!
22:32:09 From  Brian A Sakarata gl  to  Everyone:
	because explainability is so strorng
22:32:09 From  Nageswara Rao Biradhar gl  to  Everyone:
	Thank you Bradford
22:32:10 From  Chi-lieh Lin gl  to  Everyone:
	üëç
22:32:10 From  Reto Voegeli gl  to  Everyone:
	Thank you
22:32:11 From  Mohamad Khachab gl  to  Everyone:
	Thanks everyone.
22:32:11 From  Viktoriya Olari gl  to  Hosts and panelists:
	Thank you!
22:32:12 From  Alvin Kuo gl  to  Everyone:
	üôÇ Thanks
22:32:13 From  Will McGuire gl  to  Everyone:
	cheers!
22:32:13 From  RAVI KUMAR TOLETY gl  to  Everyone:
	Thanks Brad..
22:32:14 From  Wilberto W Montoya gl  to  Everyone:
	thanks
22:32:14 From  Ji Yoon Oh gl  to  Everyone:
	Thank you
22:32:14 From  Manidip Ghosh gl  to  Everyone:
	great session - thanks
22:32:14 From  Joseph W. Kinsella gl  to  Everyone:
	Thanks!
22:32:15 From  Mohammad Ayub gl  to  Hosts and panelists:
	Thanks
22:32:15 From  MARIA SUSANA REZZONICO gl  to  Everyone:
	Thank you
22:32:15 From  Jacqueline gl  to  Everyone:
	Thanks
22:32:16 From  FABIANA P NOVELLO gl  to  Everyone:
	thank you
22:32:17 From  George Katsriku gl  to  Hosts and panelists:
	Thanks
22:32:19 From  Paula Iglesias Ot√°rola gl  to  Everyone:
	Thanks!!
22:32:22 From  venkata Ratna Priya moganti gl  to  Hosts and panelists:
	Thank u
22:32:24 From  Naresh Mutyala gl  to  Everyone:
	Thank you
